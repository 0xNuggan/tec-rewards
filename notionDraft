# MODULAR R.A.D. DASHBOARD :call_me_hand: 
This a sketch of the modular design of the RAD Dashboard. The goal is to offer an automated data pipeline for DAOs to analyze their reward systems and distribute the funds in a transparent way.
The workflow will be based on a set of interconnected Jupyter Notebooks, which are fed reward data and generate a set of analytics reports and payment lists.

#### Why Jupyter notebooks?
*  **Popular:** Jupyter Notebooks are a standard in data science. This makes our system directly acessible to a great amount of people, who can adopt and expand the dashboard with little friction.
* **Modular:** Notebooks are for the most part self-contained. This makes it easy to "swap out" parts of the pipeline and isolate and debug errors. 
* **Flexible:** Notebooks support around a dozen different languanges. Each analyst can choose their on language of expertise and directly integrate with the rest of the system.
* **User-friendly:** The combination of code and markup facilitates the creation of reports which are readable by an audience without programming experience. Since the format stores the state after an execution, users dont even need to execute anything themselves. 


To tie the system together we will use *papermill* and *scrapbook*. These are open-source tools which allow for parameterized, "head-less" runs of notebooks and data exchange between them. 

### How the system works:

1. The R.A.D. Dashboard receives its data form the reward system backend database, together with a set of parameters for the distribution. 
2. This data goes into a first notebook where it is cleaned and processed, and the rewards for each user are calculated. The rewards are exported into a file of the appropriate distribution format, and the processed data is sent forward for analysis.
3. Several Notebooks are executed to analyze the data, each focusing on a particular aspect. These notebooks generate static, user-readable reports and save them, along with a copy of themselves in executed state.
4. The raw data, parameters, reports, executed-state-notebooks and distribution files are commited to Github. There they are easily auditable, since any change would affect commit history. The notebooks can be opened in-browser through tools like mybinder.org .   
	
### MVP:
For the MVP, the R.A.D Dashboard should be able to execute this process for the TEC Praise Reward System and SourceCred. The raw data will be supplied in csv format, and the results will be manually commited to GitHub. 


### Future Development possibilites: 

* Future versions could integrate directly with the backend database, allowing for automatic, scheduled execution. Commiting the results to a Github repository should also be automated.
* Expanding to support other reward systems, e.g. Coordinape.
* Since papermill and scrapbook are part of the nteract ecosystems, further nteract integrations are thinkable. Examples would be a tailored React.js-based interface for the Jupyter notebooks (practical, since we have several React devs), or a self-hosted portal to allow read-only notebook visualizations without relying on mybinder. 





### System Architecture:
```
tec-rewards
	|---- rad_main.py  [to be executed with the parameters file as argument]
	|---- distribution_tools
		|---- praise
			|---- praiseDistribution.ipynb
		|---- sourcred
			|---- sourcecredDistribution.ipynb
		. 
	|---- analysis_tools
		|---- general_tool_module.py 
		|---- praise
			|---- praise_tool_module.py 
			|---- generalPraiseReport.ipynb
			|---- praiseGiverAnalysis.ipynb
			|---- quantifierAnalysis.ipynb
		|---- sourcecred
			|---- sourcecred_tool_module.py
			|---- generalSourcecredReport.ipynb
			|---- specificSourcecredAnalyis.ipynb
		|---- coordinape
			...
		. 
	|---- data [in a production system this would be a different repository]
		|---- round1
			|---- params.json
			|---- praise.csv
			|---- sourcecred.csv
			|---- rewardboard.csv
			|---- results
				|---- output_notebooks
					|---- round1_praiseDistribution_output.ipynb
					|---- round1_generalPraiseReport_output.ipynb
					|---- round1_praiseGiverAnalysis_output.ipynb
					|---- round1_quantifierAnalysis_output.ipynb
					|---- round1_generalSourcecredReport_output.ipynb
					|---- round1_specificSourcecredAnalyis_output.ipynb
				|---- reports
					|---- generalPraiseReport.html
					|---- praiseGiverAnalysis.html
					|---- quantifierAnalysis.html
					|---- generalSourcecredReport.html
					|---- specificSourcecredAnalyis.html
				|---- distribution
					|---- final_rewards.csv
					|---- extendend_praise_distribution_data.csv
					|---- disperseApp_compatible_file.csv

		|---- round2
			...
		.
	|---- README.md					
	.

```


This structure would contain several different types of files:
* CSVs:
	* **Raw data CSVs:** To be saved in the root folder of each round. These are just exact copies of the data the backend supplied as input.
	* **Distribution CSVs:**  These save the results of the distribution in an easily exportable/importable way. Any kind of file intended to fed forward into on-chain distribution systems (like Disperse.app) also belongs to this group.
* Jupyter notebooks:
	* **Distribution notebooks:** Notbooks that take the raw data, clean&process it, generate the reward distribution and save it for the analysis step. These are not intended to be read by regular users, and will probably consist predominantly of execution cells.
	* **Analysis notebooks:** These are empty "templates" into which we feed the data generated each round by the distribution notebooks to create the reports. Each notebook is intended to focus on one relevant point in a straightforward, non-path-dependent way, and most heavy code should be outsourced to the python modules, to keep it readable. Ideally, the exection cells would more or less look like "take relevant DataFrame" -> "select/rename some columns" -> "send to a function saved in the toolbox" -> "display results"
	* **Report notebooks:** These are copies of the analysis notebooks, but filled with the data of that round, and saved in the state after their execution. Their main use will be to allow for audits, and to bind the "execution that generated the distribution" to a Github commit, to protect against tampering the data afterwards.
* Python files:
	* **Toolboxes:** Modules to be used by the notebooks. These should consist of methods which are as generalized as possible, so different notebooks can use them with different datasets. Ideally, each method should take exclusively the part of the data they need and then return a result without changing any state ("pure" methods). There should be a "general" toolbox with functions that take any kind of data in the format ['IDENTIFIER','NUMBER'], and then specific toolboxes better tailored to each reward system. 
* HTML files:
	* **Reports:** An HTML version of the report notebooks excluding the code cells, leaving a formatted, easily readable document. Storing them in HTML instead of PDF allows us to save all generated graphs in their interactive form, with hover effects, etc. In any case, HTML files can easily be printed to a regular PDF file by any modern browser if necessary. *Most of the regular users will only ever interact with these documents*   
	

## Data Analytics to implement (Work in progress):
General brainstorm of what we want to show. Every group doesn't have to be a separate notebook, although maybe it makes sense. 
:white_check_mark: denotes that the metric is already implemented somewhere
:o: denotes that it's a TO DO

* Praise:
     * "General Summary Report":
		* Amount of tokens distributed, Timeframe, praiser/praisee/quantifier numbers. :o:
		* General shape of the distribution :white_check_mark:
	* Praise Deep Dive:
		* Praise flows :white_check_mark:
		* Controversial Praise / outliers :white_check_mark:
		* Clustering of duplicate praise?:o:
	* Quantifiers:
		* Praise allocation by quantifier :white_check_mark:
		* Quantifier praise scoring boxplot :white_check_mark:
	* Praise Givers:
		* Plot praise given against praise received :o:
* Sourcered 
	* "General Summary Report" :o:
	* (More specific analytics would depend on how much data we can get out of Sourcecred)
	

## JSON Parameter file format (Work in progress):
{

}
	


	     

